---
layout:     post
title:      "支持向量机学习随笔"
subtitle:   "svm学习中的心得和笔记"
date:       2017-04-17 12:00:00
author:     "zfl"
header-img: "img/post-sklearn-svm.jpg"
header-mask: 0.6
catalog:    true
tags:
    - 机器学习
    - svm
    - 感知机
---

# 支持向量机随笔
## 前言  
一直想写一篇关于支持向量机的文章，因为它是我的机器学习生涯中第一个遇到的大难题，前后花了很长时间去研究，现在还在不断的深入学习之中，对于不求甚解的同学也可以直接下载LIBSVM或者sklearn.svm等类库使用，but，个人觉得这个算法真的特别强大、也特别有意思，思路之精妙、逻辑之严谨让我叹为观止，也向该算法的发明者以及后来的研究者致以崇高的敬意。  
本文只是本人闲暇之余做的学习笔记，难免有疏漏、不妥之处，不吝赐教。
## 一 svm简介
支持向量机（Support Vector Machine，常用简称：svm）是一种**二类分类模型**，**非线性的分类器**，算法的学习策略是**间隔最大化**，进而可以转化为求解**凸二次规划的最优化算法**。  
svm包含由简至繁的模型：**线性可分支持向量机**、**线性支持向量机**、**非线性支持向量机**，其中简单的是复杂的基础，也是复杂模型的特殊情况。
我们来找一个通俗的例子来描述这三种模型，在桌子上假如有一堆红豆和一堆黄豆：
- 1 线性可分支持向量机，如下图：
![image](http://upload-images.jianshu.io/upload_images/730879-8ac6ee89c5c75a00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)   

可以看出，通过画一条线，我们就可以很容易的把他们区分开，如果我们只是随意的、普普通通的画一条直线把二者区分开，这就是**感知机**。  
现在我们在画直线的时候使这条直线跟这些红豆、黄豆的间隔最大，那么首先最大的限制使得显然这条直线只有唯一的一条，通过**硬间隔最大化**可以得到这样一条唯一的直线。我们给这条直线加的限制就是svm区别于感知机的地方所在。  

- 2 线性支持向量机：

![svm-2.png](http://upload-images.jianshu.io/upload_images/730879-c494c19cad80522f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如图：加入有一些黄豆、红豆中的坏分子跑到了对方的大本营，使得不存在一条直线能完全正确的区分二者，但是整体又是可以区分的，这时候就要使用**软间隔最大化**来找到这一条直线。
- 3 非线性支持向量机  

![svm-3.png](http://upload-images.jianshu.io/upload_images/730879-c87aab4e598e6ec3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  

如图：红豆、黄豆就这样被人搓麻将一样搅和在了一起，再也没有可能去找一条直线将其划分开，这时，我们奋力垂直的抬起桌子，让这些豆豆都收到一个垂直向上的推力，加入我们选择的黄豆都是颗粒饱满、红豆都是干瘪的，黄豆的单个质量比较大，红豆的都偏小，黄豆的重力也就偏大，这就导致了受到同样的力黄豆上升的距离比较短，可能某一时刻，会出现下图的这种分布：
![svm-4.png](http://upload-images.jianshu.io/upload_images/730879-259697e14db6401f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  

是不是很神奇，这样就可以找一个平面把黄豆红豆划分开，上面就是把本来存在在二维里的黄豆红豆拓展到了三维空间，这种维数扩展的方法就叫**核函数**。  
总结：以上就是关于svm的关键字的直观通俗的解释，接下来就是详细的介绍，其中涉及到了很多数学公式和数学原理哦。
## 二 svm详解
### 1 感知机
感知机（perceptron）1957年由Rosenblatt提出，是**神经网络**与**支持向量机**的**基础**，在神经网络中可以看作是一个单位神经元。感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。对于线性可分的数据找到一个线性划分的超平面，相当于上面豆子例子中的找到一条线将红豆和黄豆分开，只要能分开就行，没有其他的限制。
为了得到这个超平面，使用基于误分类的损失函数，利用**梯度下降法**对损失函数进行极小化，感知机的学习算法非常简单，有**原始形式** 和 ** 对偶形式 **。  
#### 线性可分的数学形式  
给定一个数据集$$ T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\} $$，其中$$x_i\in X = R^n, y_i \in \{-1,+1\} , i = 1,2,3...N $$,如果存在某个超平面$$ wx + b = 0 $$能够将正实例点和负实例点都正确的划分到超平面的两侧，即对于所有的$$ y_i > 0 $$的实例i，有$$ wx_i + b > 0 $$,对于所有的 $$ y_i < 0 $$的实例i，都有$$ wx_i +b < 0 $$，也可以说对于所有的实例i，都有$$ y_i(wx_i+b) > 0 $$。  
#### 感知机模型  
感知机是针对线性可分的数据集的，从输入空间到输出空间的函数为：  

$$
    f(x) = sign(wx + b)
$$  

其中，$$ wx + b = 0 $$就是一个超平面，其中w是超平面的法向量，b是超平面的截距，  

$$
f(x)=\begin{cases} 1, x > 0 \\ -1,y<0  \end{cases}
$$  

感知机的学习策略就是找到这样一个超平面S，也就是求得函数的参数w、b，这是感知机的几何解释。  
**感知机的另一种解释：**类似于生物上神经元的信息处理工具，对于特征向量的特征值$$ x_1,x_2,...,x_N $$，对应的权值w有$$ w_1,w_2,...,w_N $$表示了每个特征的重要程度，感知机的输出到底是-1或者+1还要有一个偏置，本文感知机的偏置为-b，如果特征值和权值的加权和$$ \sum_{i=1}^N w_ix_i $$大于偏置-b，则为+1，如果加权和小于偏置-b则为-1；如下图：
![svm-5.png](http://upload-images.jianshu.io/upload_images/730879-b1c28fa6af6be62d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  
怎么样求得函数的参数呢？我们去找一个损失函数，损失函数是关于w、b的连续可导函数，由此我们使用选择所有误分类点到超平面S的距离之和，任意一点到超平面的距离为：  

$$
    \frac{1}{\Vert w \Vert} | wx + b|
$$  

其中，$$  \Vert w  \Vert $$是w的 $$ L_2 $$范数。对于误分类点$$ y_i(wx_i+b) < 0 $$,因此误分类点到超平面S的距离为：  

$$
-\frac{1}{\Vert w \Vert} y_i( wx + b)
$$  

假设所有误分类集合为M，那么所有误分类点到超平面S的距离之和为：  

$$
    -\frac{1}{ \Vert w \Vert} \sum_{x_i \in M}y_i( wx + b)
$$  

不考虑$$ \frac{1}{\Vert w \Vert} $$就得到了感知机学习的损失函数。这里为啥不考虑这个参数呢，因为没有必要啦，在介绍支持向量机的时候会进行说明。   
**感知机的损失函数为：**  

$$
    L(w,b) = - \sum_{x_i \in M}y_i( wx + b)
$$    

#### 学习算法
没有办法直接求得参数w、b，我们就转化为求损失函数极小化问题的解，即：  

$$
    min_{w,b}L(w,b) = - \sum_{x_i \in M}y_i( wx + b)
$$  

采用随机梯度下降算法（这个算法特别重要，要好好的深入了解），任意选择一个超平面$$ w_0,b_0$$,然后随机选择一个误分类点进行梯度下降。  
损失函数两个参数的梯度分别为：  

$$
    \nabla w = - \sum_{x_i \in M} y_i x_i  
$$  
  

$$
    \nabla b = -\sum _{x_i \in M} y_i
$$  

随机选择一个误分类点$$ (x_i,y_i) $$,对w、b更新：  

$$
    w \leftarrow w + \eta y_i x_i
$$  


$$
    b \leftarrow b + \eta y_i
$$   

其中，$$ \eta (0 < \eta <= 1) $$为步长，在统计学习中我们又称之为学习率。梯度下降算法可以很形象的理解为下山，我们站在山上的某个地方，梯度方向就是所在位置最陡的方向，也就是目前位置下山最快的方向；我们为了下山最快，就从当前位置开始，先沿着梯度方向下山一段距离，然后再从当前位置沿着当前位置的梯度方向再下山一段距离，直到下山。这个η就是我们沿着梯度方向每次走的距离。如果η太大，可能会错过凹曲线的最低点，引发震荡（在最小点附近来回走）；如果η太小，则影响效率。
# <未完待续>